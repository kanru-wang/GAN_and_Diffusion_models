{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdM0vjqNbtHhpqLCdqyLTY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanru-wang/Udemy_GAN_and_Diffusion_models/blob/main/CelebA_SRGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CelebA SRGAN\n",
        "\n",
        "Super Resolution GAN trained on the CelebA dataset for generating facial images."
      ],
      "metadata": {
        "id": "eA_XPEROxlyq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rzJ4TKrxSGK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Layer, Reshape, LeakyReLU, Dropout, Conv2DTranspose, Add, Conv2D, MaxPool2D,\n",
        "    Dense, Flatten, InputLayer, BatchNormalization, Input, GlobalAvgPool2D, PReLU\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "Z12OmMmIyBfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Download\n",
        "\n",
        "Before running the following cell, must first log into Kaggle, go to `Settings -> API -> Create New Token`.\n",
        "\n",
        "Download `kaggle.json` and upload it to the root folder of Colab."
      ],
      "metadata": {
        "id": "AzkOWcHzyQrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# suppress output\n",
        "%%capture\n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d badasstechie/celebahq-resized-256x256\n",
        "! unzip \"/content/celebahq-resized-256x256.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "id": "M7oiTNb9x8LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "KaV39ULQyZNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "IM_SHAPE = (64, 64, 3)\n",
        "B = 24\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/dataset/celeba_hq_256\",\n",
        "    label_mode=None,\n",
        "    image_size=(IM_SHAPE[0], IM_SHAPE[1]),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "ds"
      ],
      "metadata": {
        "id": "EYYd-iGNx8I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image):\n",
        "    return (\n",
        "        tf.image.resize(image, [IM_SHAPE[0] // 4, IM_SHAPE[1] // 4], method='bicubic') / 255,\n",
        "        tf.cast(image, tf.float32) / 127.5 - 1.0\n",
        "    )\n",
        "\n",
        "train_dataset = (\n",
        "    ds.take(12000)\n",
        "    .map(preprocess)\n",
        "    .unbatch()\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "for d1, d2 in train_dataset.take(1):\n",
        "    print(d1.shape, d2.shape)"
      ],
      "metadata": {
        "id": "w2UcgUnAx8Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "k = 0\n",
        "n = 6\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(3, 2, k + 1)\n",
        "    if i < 2:\n",
        "        plt.imshow(d1[i])\n",
        "    elif i >= 2 and i < 4:\n",
        "        plt.imshow(cv2.resize(np.array(d1[i - 2]), (64, 64)))\n",
        "    else:\n",
        "        plt.imshow((d2[i - 4] + 1) / 2)\n",
        "    plt.axis(\"off\")\n",
        "    k += 1"
      ],
      "metadata": {
        "id": "EVWge-MDx8ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "sOvGlFyw41Lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "rHs1fil9WjCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(Layer):\n",
        "    def __init__(self, n_filters, filter_size, strides, name='res_block'):\n",
        "        super(ResBlock, self).__init__(name=name)\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.strides = strides\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv_1 = Conv2D(self.n_filters, self.filter_size, strides=self.strides, padding='same')\n",
        "        self.batch_norm_1 = BatchNormalization()\n",
        "        self.prelu = PReLU()\n",
        "        self.conv_2 = Conv2D(self.n_filters, self.filter_size, strides=self.strides, padding='same')\n",
        "        self.batch_norm_2 = BatchNormalization()\n",
        "\n",
        "    def call(self, x_in):\n",
        "        x = self.conv_1(x_in)\n",
        "        x = self.prelu(self.batch_norm_1(x))\n",
        "        x = self.conv_2(x)\n",
        "        x = self.batch_norm_2(x)\n",
        "        return x + x_in\n",
        "\n",
        "\n",
        "class UpsampleBlock(Layer):\n",
        "    def __init__(self, n_filters, filter_size, strides, name='upsample_block'):\n",
        "        super(UpsampleBlock, self).__init__(name=name)\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.strides = strides\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv = Conv2D(self.n_filters, self.filter_size, strides=self.strides, padding='same')\n",
        "        self.prelu = PReLU()\n",
        "\n",
        "    def call(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = tf.nn.depth_to_space(x, 2)\n",
        "        x = self.prelu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YzQWND5zx8CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lr = tf.keras.layers.Input(shape=(IM_SHAPE[0] // 4, IM_SHAPE[1] // 4, 3))\n",
        "input_conv = tf.keras.layers.Conv2D(64, 9, 1, padding='same')(input_lr)\n",
        "input_conv = PReLU()(input_conv)\n",
        "\n",
        "x = input_conv\n",
        "for i in range(B):\n",
        "    x = ResBlock(64, 3, 1, name='res_block_' + str(i))(x)\n",
        "x = tf.keras.layers.Conv2D(64, 9, padding='same')(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "x += input_conv\n",
        "\n",
        "x = UpsampleBlock(256, 3, 1, name='upsample_block_1')(x)\n",
        "x = UpsampleBlock(256, 3, 1, name='upsample_block_2')(x)\n",
        "output_sr = tf.keras.layers.Conv2D(3, 9, activation='tanh', padding='same')(x)\n",
        "\n",
        "srresnet = tf.keras.models.Model(input_lr, output_sr)\n",
        "srresnet.summary()"
      ],
      "metadata": {
        "id": "r2h7lgOJx7_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "4ltUGHFlWpC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(Layer):\n",
        "    def __init__(self, n_filters, filter_size, strides, name='conv_block'):\n",
        "        super(ConvBlock, self).__init__(name=name)\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.strides = strides\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv = Conv2D(self.n_filters, self.filter_size, strides=self.strides, padding='same')\n",
        "        self.batch_norm = BatchNormalization()\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = LeakyReLU()(self.batch_norm(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "wbs3ZN3Xx79q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lr = tf.keras.layers.Input(shape=(IM_SHAPE[0], IM_SHAPE[1], 3))\n",
        "input_conv = tf.keras.layers.Conv2D(64, 3, padding='same')(input_lr)\n",
        "input_conv = tf.keras.layers.LeakyReLU()(input_conv)\n",
        "\n",
        "channel_nums = [64, 128, 128, 256, 256, 512, 512]\n",
        "stride_sizes = [2, 1, 2, 1, 2, 1, 2]\n",
        "\n",
        "disc = input_conv\n",
        "for i in range(7):\n",
        "    disc=ConvBlock(channel_nums[i], 3, stride_sizes[i], name='conv_block_' + str(i))(disc)\n",
        "disc = GlobalAvgPool2D()(disc)\n",
        "disc = tf.keras.layers.Dense(1024)(disc)\n",
        "disc = tf.keras.layers.LeakyReLU()(disc)\n",
        "\n",
        "disc_output = tf.keras.layers.Dense(1, activation='sigmoid')(disc)\n",
        "\n",
        "discriminator = tf.keras.models.Model(input_lr, disc_output)\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "id": "K2kGxddQx77e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Together"
      ],
      "metadata": {
        "id": "XzL711ypZIUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VGG19 = tf.keras.applications.VGG19(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(256, 256, 3)\n",
        ")\n",
        "\n",
        "\n",
        "def VGG_loss(y_hr, y_sr, i_m=2, j_m=2):\n",
        "    i, j = 0, 0\n",
        "    accumulated_loss = 0\n",
        "    for l in VGG19.layers:\n",
        "        cl_name = l.__class__.__name__\n",
        "        if cl_name == 'Conv2D':\n",
        "            j += 1\n",
        "        if cl_name == 'MaxPooling2D':\n",
        "            i += 1\n",
        "            j = 0\n",
        "        if i == i_m and j == j_m:\n",
        "            break\n",
        "        y_hr = l(y_hr)\n",
        "        y_sr = l(y_sr)\n",
        "        if cl_name == 'Conv2D':\n",
        "            mse = tf.keras.losses.MeanSquaredError(name='mean_squared_error')\n",
        "            accumulated_loss += mse(y_hr, y_sr) * 0.006\n",
        "    return accumulated_loss\n",
        "\n",
        "\n",
        "def content_loss(y_true,y_pred):\n",
        "    mse = tf.keras.losses.MeanSquaredError(name='mean_squared_error')\n",
        "    return mse(y_true, y_pred) + VGG_loss(y_true, y_pred)\n",
        "\n",
        "\n",
        "class GANMonitor(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        plt.figure(figsize=(16, 16))\n",
        "        k = 0\n",
        "        n = 6\n",
        "        for i in range(n):\n",
        "            ax = plt.subplot(3, 2, k+1)\n",
        "            if i < 2:\n",
        "                plt.imshow(d1[i])\n",
        "            elif i >= 2 and i < 4:\n",
        "                out = self.model.generator(tf.expand_dims(d1[i - 2], axis=0))\n",
        "                plt.imshow((out[0] + 1) / 2)\n",
        "            else:\n",
        "                plt.imshow((d2[i - 4] + 1) / 2)\n",
        "            plt.axis(\"off\")\n",
        "            k += 1\n",
        "        plt.savefig(\"generated/gen_images_epoch_{}.png\".format(epoch + 1))\n",
        "\n",
        "\n",
        "class SRGAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator):\n",
        "        super(SRGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_disc, loss_gen):\n",
        "        super(SRGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_disc = loss_disc\n",
        "        self.loss_gen = loss_gen\n",
        "        self.d_loss_metric = tf.keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = tf.keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        lr_images, hr_images = real_images\n",
        "        batch_size = tf.shape(hr_images)[0]\n",
        "        generated_images = self.generator(lr_images)\n",
        "        real_labels = tf.ones((batch_size, 1))\n",
        "        fake_labels = tf.zeros((batch_size, 1))\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            real_predictions = self.discriminator(hr_images)\n",
        "            d_loss_real = self.loss_disc(real_labels, real_predictions)\n",
        "            fake_predictions = self.discriminator(generated_images)\n",
        "            d_loss_fake = self.loss_disc(fake_labels, fake_predictions)\n",
        "            d_loss = 0.5 * (d_loss_fake + d_loss_real)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
        "        misleading_labels = tf.ones((batch_size, 1))\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.generator(lr_images)\n",
        "            g_loss = self.loss_gen(hr_images, predictions)\n",
        "            g_loss = g_loss + 1e-3 * self.loss_disc(misleading_labels, self.discriminator(predictions))\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "02f9bssUx75O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "SEausWW9dQjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 36\n",
        "\n",
        "gan = SRGAN(discriminator=discriminator, generator=srresnet)\n",
        "gan.compile(\n",
        "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5),\n",
        "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5),\n",
        "    loss_disc=tf.keras.losses.BinaryCrossentropy(),\n",
        "    loss_gen=content_loss\n",
        ")\n",
        "\n",
        "!mkdir generated\n",
        "\n",
        "history = gan.fit(train_dataset, epochs=epochs, callbacks=[GANMonitor()])"
      ],
      "metadata": {
        "id": "5Ml37W4Qx729"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}