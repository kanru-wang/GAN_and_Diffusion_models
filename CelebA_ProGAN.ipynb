{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqmDuI1cWQLvpEkiGe+jVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanru-wang/Udemy_GAN_and_Diffusion_models/blob/main/CelebA_ProGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CelebA ProGAN\n",
        "\n",
        "Progressive Growing GAN trained on the CelebA dataset for generating facial images."
      ],
      "metadata": {
        "id": "idWVIO551AHj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al9ImjdvNe16"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Layer, Reshape, LeakyReLU, Dropout, Conv2DTranspose, Add, AveragePooling2D,\n",
        "    Lambda, UpSampling2D, Conv2D, MaxPool2D, Dense, Activation, Flatten,\n",
        "    InputLayer, BatchNormalization, Input\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "DvxmHKdXNi5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "## Data Download\n",
        "\n",
        "Before running the following cell, must first log into Kaggle, go to `Settings -> API -> Create New Token`.\n",
        "\n",
        "Download `kaggle.json` and upload it to the root folder of Colab."
      ],
      "metadata": {
        "id": "p60IXMgMN0s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# suppress output\n",
        "%%capture\n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d badasstechie/celebahq-resized-256x256\n",
        "! unzip \"/content/celebahq-resized-256x256.zip\" -d \"/content/dataset/\"\n",
        ""
      ],
      "metadata": {
        "id": "r-azqF4INi3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "kfcLvVRaOKcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NOISE_DIM = 512\n",
        "BATCH_SIZE = [16, 16, 16, 16, 16, 16]\n",
        "EPOCHS = 50\n",
        "FILTERS = [512, 512, 512, 512, 256, 128, 64]\n",
        "\n",
        "def preprocess(image):\n",
        "    return tf.cast(image, tf.float32) / 127.5 - 1.0\n",
        "\n",
        "def create_dataset(res,BATCH_SIZE):\n",
        "    ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        \"/content/dataset/celeba_hq_256\",\n",
        "        label_mode=None,\n",
        "        image_size=(res, res),\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "    train_dataset = (\n",
        "        ds_train\n",
        "        .map(preprocess)\n",
        "        .unbatch()\n",
        "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
        "        .batch(BATCH_SIZE, drop_remainder=True)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return train_dataset\n",
        "\n",
        "for d in create_dataset(256, 32).take(1):\n",
        "    print(d.shape)"
      ],
      "metadata": {
        "id": "ZDkN0_G-Ni0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "k = 0\n",
        "n = 16\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(4, 4, k + 1)\n",
        "    plt.imshow((d[i] + 1) / 2)\n",
        "    plt.axis(\"off\")\n",
        "    k += 1"
      ],
      "metadata": {
        "id": "U52UMW4yNix8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling\n",
        "\n",
        "<img src=\"https://miro.medium.com/1*lStHChxfyLB3S7wUW3Quiw.png\" width=\"700\"/>\n",
        "\n",
        "<img src=\"https://miro.medium.com/1*SoWghLQBfcW5i7tAuqQamQ.png\" width=\"700\"/>\n",
        "\n",
        "https://towardsdatascience.com/progan-how-nvidia-generated-images-of-unprecedented-quality-51c98ec2cbd2"
      ],
      "metadata": {
        "id": "-rYSxHmKpxcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelNormalization(Layer):\n",
        "    def __init__(self):\n",
        "        super(PixelNormalization, self).__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean_square = tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True)\n",
        "        l2 = tf.math.rsqrt(mean_square + 1.0e-8)\n",
        "        normalized = inputs * l2\n",
        "        return normalized\n",
        "\n",
        "\n",
        "class WeightScaling(Layer):\n",
        "    \"\"\"\n",
        "    Equalized Learning Rate: Before every forward pass during training, scale\n",
        "    the weights of a layer according to how many weights that layer has.\n",
        "    \"\"\"\n",
        "    def __init__(self, shape, gain=np.sqrt(2)):\n",
        "        super(WeightScaling, self).__init__()\n",
        "        # shape = tf.constant(shape, dtype=tf.float32)\n",
        "        fan_in = tf.math.reduce_prod(shape)\n",
        "        self.wscale = gain * tf.math.rsqrt(fan_in)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, tf.float32)\n",
        "        return inputs * self.wscale\n",
        "\n",
        "\n",
        "class Bias(Layer):\n",
        "    def __init__(self):\n",
        "        super(Bias, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.bias = tf.Variable(\n",
        "            initial_value=b_init(shape=(input_shape[-1],), dtype=\"float32\"),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.bias\n",
        "\n",
        "\n",
        "class WeightScalingDense(Layer):\n",
        "    def __init__(self, n_units, gain, use_pixelnorm=False, activate=None):\n",
        "        super(WeightScalingDense, self).__init__()\n",
        "        self.n_units = n_units\n",
        "        self.gain = gain\n",
        "        self.use_pixelnorm = use_pixelnorm\n",
        "        self.activate = activate\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.dense = Dense(\n",
        "            self.n_units,\n",
        "            use_bias=False,  # Do Weight Scaling without bias; then add bias initialized as zeros\n",
        "            kernel_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=1.),\n",
        "            dtype=\"float32\"\n",
        "        )\n",
        "        self.bias = Bias()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        in_filters = tf.shape(inputs)[-1]\n",
        "        x = self.dense(inputs)\n",
        "        x = WeightScaling(shape=(tf.cast(in_filters, dtype=tf.float32)), gain=self.gain)(x)\n",
        "        x = self.bias(x)\n",
        "        if self.activate == \"LeakyReLU\":\n",
        "            x = LeakyReLU(0.2)(x)\n",
        "        elif self.activate == \"tanh\":\n",
        "            x = Activation(\"tanh\")(x)\n",
        "        if self.use_pixelnorm:\n",
        "            x = PixelNormalization()(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class WeightScalingConv(Layer):\n",
        "    def __init__(self, n_filters, kernel_size, gain, use_pixelnorm=False, activate=None, strides=(1,1)):\n",
        "        super(WeightScalingConv, self).__init__()\n",
        "        self.n_filters = n_filters\n",
        "        self.gain = gain\n",
        "        self.kernel_size = kernel_size\n",
        "        self.use_pixelnorm = use_pixelnorm\n",
        "        self.activate = activate\n",
        "        self.strides = strides\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv = Conv2D(\n",
        "            self.n_filters,\n",
        "            self.kernel_size,\n",
        "            strides=self.strides,\n",
        "            use_bias=False,  # Do Weight Scaling without bias; then add bias initialized as zeros\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=1.),\n",
        "            dtype=\"float32\"\n",
        "        )\n",
        "        self.bias = Bias()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        in_filters = tf.shape(inputs)[-1]\n",
        "        x = self.conv(inputs)\n",
        "        x = WeightScaling(\n",
        "            shape=(\n",
        "                tf.cast(self.kernel_size[0], dtype=tf.float32),\n",
        "                tf.cast(self.kernel_size[1], dtype=tf.float32),\n",
        "                tf.cast(in_filters, dtype=tf.float32)\n",
        "            ),\n",
        "            gain=self.gain\n",
        "        )(x)\n",
        "        x = self.bias(x)\n",
        "        if self.activate == \"LeakyReLU\":\n",
        "            x = LeakyReLU(0.2)(x)\n",
        "        elif self.activate == \"tanh\":\n",
        "            x = Activation(\"tanh\")(x)\n",
        "        if self.use_pixelnorm:\n",
        "            x = PixelNormalization()(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MinibatchStdev(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MinibatchStdev, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):  # assume inputs.shape = (4, 4, 4, 512)\n",
        "        mean = tf.reduce_mean(inputs, axis=0, keepdims=True)  # (1, 4, 4, 512)\n",
        "        stddev = tf.sqrt(tf.reduce_mean(tf.square(inputs - mean), axis=0, keepdims=True) + 1e-8)  # (1, 4, 4, 512)\n",
        "        average_stddev = tf.reduce_mean(stddev, keepdims=True)  # (1, 1, 1, 1)\n",
        "        shape = tf.shape(inputs)\n",
        "        minibatch_stddev = tf.tile(average_stddev, (shape[0], shape[1], shape[2], 1))  # (4, 4, 4, 1)\n",
        "        combined = tf.concat([inputs, minibatch_stddev], axis=-1)  # (4, 4, 4, 513)\n",
        "        return combined\n",
        "\n",
        "\n",
        "class WeightedSum(Layer):\n",
        "    def __init__(self):\n",
        "        super(WeightedSum, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.alpha = tf.Variable(0., dtype=tf.float32, trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return ((1.0 - self.alpha) * inputs[0] + (self.alpha * inputs[1]))\n",
        "\n",
        "\n",
        "class ProGAN(Model):\n",
        "    def __init__(self, latent_dim, d_steps=1, gp_weight=10.0, drift_weight=0.001):\n",
        "        super(ProGAN, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_steps = d_steps\n",
        "        self.gp_weight = gp_weight\n",
        "        self.drift_weight = drift_weight\n",
        "        self.n_depth = 0\n",
        "        self.discriminator = self.init_discriminator()\n",
        "        self.discriminator_wt_fade = None\n",
        "        self.generator = self.init_generator()\n",
        "        self.generator_wt_fade = None\n",
        "\n",
        "    def init_discriminator(self):\n",
        "        img_input = Input(shape=(4, 4, 3))\n",
        "        img_input = tf.cast(img_input, tf.float32)\n",
        "        # fromRGB\n",
        "        x = WeightScalingConv(n_filters=FILTERS[0], kernel_size=(1, 1), gain=np.sqrt(2),\n",
        "                              activate=\"LeakyReLU\")(img_input)\n",
        "        x = MinibatchStdev()(x)\n",
        "        x = WeightScalingConv(n_filters=FILTERS[0], kernel_size=(3, 3), gain=np.sqrt(2),\n",
        "                              activate=\"LeakyReLU\")(x)\n",
        "        x = WeightScalingConv(n_filters=FILTERS[0], kernel_size=(4, 4), gain=np.sqrt(2),\n",
        "                              activate=\"LeakyReLU\", strides=(4, 4))(x)\n",
        "        x = Flatten()(x)\n",
        "        x = WeightScalingDense(n_units=1, gain=1.)(x)\n",
        "        d_model = Model(img_input, x, name=\"discriminator\")\n",
        "        return d_model\n",
        "\n",
        "    def fade_in_discriminator(self):\n",
        "        input_shape = list(self.discriminator.input.shape)\n",
        "        input_shape = (input_shape[1] * 2, input_shape[2] * 2, input_shape[3])\n",
        "        img_input = Input(shape=input_shape)\n",
        "        img_input = tf.cast(img_input, tf.float32)\n",
        "        x1 = AveragePooling2D()(img_input)\n",
        "        x1 = self.discriminator.layers[1](x1)\n",
        "        x2 = WeightScalingConv(FILTERS[self.n_depth], (1, 1), np.sqrt(2),\n",
        "                               activate=\"LeakyReLU\")(img_input)\n",
        "        x2 = WeightScalingConv(FILTERS[self.n_depth], (3, 3), np.sqrt(2),\n",
        "                               activate=\"LeakyReLU\")(x2)\n",
        "        x2 = WeightScalingConv(FILTERS[self.n_depth - 1], (3, 3), np.sqrt(2),\n",
        "                               activate=\"LeakyReLU\")(x2)\n",
        "        x2 = AveragePooling2D()(x2)\n",
        "        x = WeightedSum()([x1, x2])\n",
        "        for i in range(2, len(self.discriminator.layers)):\n",
        "            x2 = self.discriminator.layers[i](x2)\n",
        "        self.discriminator_stabilize = Model(img_input, x2, name=\"discriminator\")\n",
        "        for i in range(2, len(self.discriminator.layers)):\n",
        "            x = self.discriminator.layers[i](x)\n",
        "        self.discriminator = Model(img_input, x, name=\"discriminator\")\n",
        "        self.discriminator.summary()\n",
        "\n",
        "    def stabilize_discriminator(self):\n",
        "        self.discriminator = self.discriminator_stabilize\n",
        "        self.discriminator.summary()\n",
        "\n",
        "    def init_generator(self):\n",
        "        noise = Input(shape=(self.latent_dim))\n",
        "        x = PixelNormalization()(noise)\n",
        "        x = WeightScalingDense(n_units=4 * 4 * FILTERS[0], gain=np.sqrt(2) / 4,\n",
        "                               activate=\"LeakyReLU\", use_pixelnorm=True)(x)\n",
        "        x = Reshape((4, 4, FILTERS[0]))(x)\n",
        "        x = WeightScalingConv(FILTERS[0], kernel_size=(4, 4), gain=np.sqrt(2),\n",
        "                              activate=\"LeakyReLU\", use_pixelnorm=True)(x)\n",
        "        x = WeightScalingConv(FILTERS[0], kernel_size=(3, 3), gain=np.sqrt(2),\n",
        "                              activate=\"LeakyReLU\", use_pixelnorm=True)(x)\n",
        "        x = WeightScalingConv(3, kernel_size=(1, 1), gain=1, activate=\"tanh\",\n",
        "                              use_pixelnorm=False)(x)\n",
        "        g_model = Model(noise, x, name=\"generator\")\n",
        "        g_model.summary()\n",
        "        return g_model\n",
        "\n",
        "    def fade_in_generator(self):\n",
        "        block_end = self.generator.layers[-2].output\n",
        "        block_end = UpSampling2D((2, 2))(block_end)\n",
        "        x1 = self.generator.layers[-1](block_end)\n",
        "        x2 = WeightScalingConv(FILTERS[self.n_depth], (3, 3), np.sqrt(2),\n",
        "                               activate=\"LeakyReLU\", use_pixelnorm=True)(block_end)\n",
        "        x2 = WeightScalingConv(FILTERS[self.n_depth], (3, 3), np.sqrt(2),\n",
        "                               activate=\"LeakyReLU\", use_pixelnorm=True)(x2)\n",
        "        x2 = WeightScalingConv(3, (1, 1), 1, activate=\"tanh\", use_pixelnorm=False)(x2)\n",
        "        self.generator_stabilize = Model(self.generator.input, x2, name=\"generator\")\n",
        "        x = WeightedSum()([x1, x2])\n",
        "        self.generator = Model(self.generator.input, x, name=\"generator\")\n",
        "        self.generator.summary()\n",
        "\n",
        "    def stabilize_generator(self):\n",
        "        self.generator = self.generator_stabilize\n",
        "        self.generator.summary()\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer):\n",
        "        super(ProGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "        epsilon = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "        interpolation = epsilon * real_images + (1 - epsilon) * fake_images\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolation)\n",
        "            prediction = self.discriminator(interpolation, training=True)\n",
        "        grads = gp_tape.gradient(prediction, interpolation)\n",
        "        l2_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        return tf.reduce_mean((l2_norm - 1) ** 2)\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_latent_vectors, training=True)\n",
        "            fake_logits = self.discriminator(fake_images, training=True)\n",
        "            real_logits = self.discriminator(real_images, training=True)\n",
        "            d_cost = tf.reduce_mean(fake_logits) - tf.reduce_mean(real_logits)\n",
        "            gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
        "            drift = tf.reduce_mean(tf.square(real_logits))\n",
        "            d_loss = d_cost + self.gp_weight * gp + self.drift_weight * drift\n",
        "        d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_images = self.generator(random_latent_vectors, training=True)\n",
        "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
        "            g_loss = -tf.reduce_mean(gen_img_logits)\n",
        "        g_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        self.g_optimizer.apply_gradients(zip(g_gradient, self.generator.trainable_variables))\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
        "\n",
        "\n",
        "class ShowImage(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, prefix, res, latent_dim=512, steps_per_epoch=1000, epochs=50):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.prefix = prefix\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.epochs = epochs\n",
        "        self.res = res\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.n_epoch = epoch\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        n=6\n",
        "        k=0\n",
        "        out=self.model.generator(tf.random.normal(shape=(36, self.latent_dim)))\n",
        "        plt.figure(figsize=(16, 16))\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                ax=plt.subplot(n, n, k + 1)\n",
        "                plt.imshow((out[k] + 1) / 2,)\n",
        "                plt.axis(\"off\")\n",
        "                k += 1\n",
        "        plt.savefig(\"generated/gen_images_{}x{}_{}_epoch_{}.png\".format(self.res, self.res, self.prefix, epoch + 1))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        alpha = (\n",
        "            ((self.n_epoch * self.steps_per_epoch) + batch)\n",
        "            / float(self.steps_per_epoch * self.epochs - 1)\n",
        "        )\n",
        "        for layer in self.model.generator.layers:\n",
        "            if isinstance(layer, WeightedSum):\n",
        "            layer.alpha.assign(alpha)\n",
        "        for layer in self.model.discriminator.layers:\n",
        "            if isinstance(layer, WeightedSum):\n",
        "            layer.alpha.assign(alpha)\n"
      ],
      "metadata": {
        "id": "0RJ3iqHLNivh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5DiqDJfizaet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir generated\n",
        "\n",
        "STEPS_PER_EPOCH = 1000\n",
        "EPOCHS = 50\n",
        "NOISE_DIM = 512\n",
        "\n",
        "train_dataset = create_dataset(4, BATCH_SIZE[0])\n",
        "generator_optimizer = Adam(learning_rate=0.001, beta_1=0.0, beta_2=0.99, epsilon=1e-8)\n",
        "discriminator_optimizer = Adam(learning_rate=0.001, beta_1=0.0, beta_2=0.99, epsilon=1e-8)\n",
        "\n",
        "pgan = ProGAN(\n",
        "    latent_dim=NOISE_DIM,\n",
        "    d_steps=1,\n",
        ")\n",
        "\n",
        "pgan.compile(\n",
        "    d_optimizer=discriminator_optimizer,\n",
        "    g_optimizer=generator_optimizer,\n",
        ")\n",
        "\n",
        "cbk = ShowImage(\"initial\", 4, latent_dim=512, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS)\n",
        "pgan.fit(train_dataset.take(STEPS_PER_EPOCH), epochs = EPOCHS, callbacks=[cbk])"
      ],
      "metadata": {
        "id": "u6OYQO7RNitR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n_depth in range(1, 6):\n",
        "    ### Faded ###\n",
        "    pgan.n_depth = n_depth\n",
        "    train_dataset = create_dataset(4 * (2 ** n_depth), BATCH_SIZE[n_depth])\n",
        "    cbk = ShowImage(\"fading\", 4 * (2 ** n_depth), latent_dim=512, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS)\n",
        "    pgan.fade_in_generator()\n",
        "    pgan.fade_in_discriminator()\n",
        "    pgan.compile(\n",
        "        d_optimizer=discriminator_optimizer,\n",
        "        g_optimizer=generator_optimizer,\n",
        "    )\n",
        "    pgan.fit(train_dataset.take(STEPS_PER_EPOCH), epochs = EPOCHS, callbacks=[cbk])\n",
        "    ### Stabilized ###\n",
        "    cbk = ShowImage(\"stabilize\", 4 * (2 ** n_depth), latent_dim=512, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS)\n",
        "    pgan.stabilize_generator()\n",
        "    pgan.stabilize_discriminator()\n",
        "    pgan.compile(\n",
        "        d_optimizer=discriminator_optimizer,\n",
        "        g_optimizer=generator_optimizer,\n",
        "    )\n",
        "    pgan.fit(train_dataset.take(STEPS_PER_EPOCH), epochs=EPOCHS, callbacks=[cbk])\n",
        "\n",
        "pgan.generator.save_weights(\"pggan_generator_128.h5\")\n",
        "pgan.discriminator.save_weights(\"pggan_discriminator_128.h5\")"
      ],
      "metadata": {
        "id": "MXZNF15WNiqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# See Details"
      ],
      "metadata": {
        "id": "dN91Q6MEzXGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_discriminator():\n",
        "    img_input = Input(shape=(4, 4, 3))\n",
        "    img_input = tf.cast(img_input, tf.float32)\n",
        "    # fromRGB\n",
        "    x = WeightScalingConv(n_filters=FILTERS[0], kernel_size=(1, 1), gain=np.sqrt(2),\n",
        "                          activate=\"LeakyReLU\")(img_input)\n",
        "    x = MinibatchStdev()(x)\n",
        "    x = WeightScalingConv(n_filters=FILTERS[0], kernel_size=(3, 3), gain=np.sqrt(2),\n",
        "                          activate=\"LeakyReLU\")(x)\n",
        "    x = WeightScalingConv(n_filters=FILTERS[0], kernel_size=(4, 4), gain=np.sqrt(2),\n",
        "                          activate=\"LeakyReLU\", strides=(4, 4))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = WeightScalingDense(n_units=1, gain=1.)(x)\n",
        "    d_model = Model(img_input, x, name=\"discriminator\")\n",
        "    return d_model\n",
        "\n",
        "init_discriminator().summary()"
      ],
      "metadata": {
        "id": "akjOj5vLNioc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_generator():\n",
        "    noise = Input(shape=(512))\n",
        "    x = PixelNormalization()(noise)\n",
        "    x = WeightScalingDense(n_units=4 * 4 * FILTERS[0], gain=np.sqrt(2) / 4,\n",
        "                           activate=\"LeakyReLU\", use_pixelnorm=True)(x)\n",
        "    x = Reshape((4, 4, FILTERS[0]))(x)\n",
        "    x = WeightScalingConv(FILTERS[0], kernel_size=(4, 4), gain=np.sqrt(2),\n",
        "                          activate=\"LeakyReLU\", use_pixelnorm=True)(x)\n",
        "    x = WeightScalingConv(FILTERS[0], kernel_size=(3, 3), gain=np.sqrt(2),\n",
        "                          activate=\"LeakyReLU\", use_pixelnorm=True)(x)\n",
        "    x = WeightScalingConv(3, kernel_size=(1, 1), gain=1, activate=\"tanh\",\n",
        "                          use_pixelnorm=False)(x)\n",
        "    g_model = Model(noise, x, name=\"generator\")\n",
        "    return g_model\n",
        "\n",
        "init_generator().summary()"
      ],
      "metadata": {
        "id": "V5pw6nXDNihi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}